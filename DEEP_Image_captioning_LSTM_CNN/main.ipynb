{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gis-local/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gis-local/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "import os;  \n",
    "from ipynb.fs.full.extractfeatures import extract_features;\n",
    "from ipynb.fs.full.textPreprocessor import processText;\n",
    "from ipynb.fs.full.testData import load_clean_descriptions_photo_features;\n",
    "from ipynb.fs.full.testData import create_tokenizer_max_length, create_sequences;\n",
    "from ipynb.fs.full.model import define_model;\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create image feature file and cleaned description of image file\n",
    "if (os.path.exists(\"./features.pkl\") ==  False):\n",
    "    extract_features(\"../../flickr8k_dataset/Flickr8k_Dataset/Flicker8k_Dataset\", \"./features.pkl\")\n",
    "if (os.path.exists(\"./descriptions.txt\") ==  False):\n",
    "    vocabulary = processText(\"../../flickr8k_dataset/Flickr8k_text/Flickr8k.token.txt\", \"./descriptions.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptions: train=6000\n",
      "Photos: train=6000\n",
      "Vocabulary Size: 7579\n",
      "Description Length: 34\n"
     ]
    }
   ],
   "source": [
    "# prepare train dataset with startseq desc endseq.\n",
    "train_descriptions, train_features  = load_clean_descriptions_photo_features(\"./descriptions.txt\", \"/home/gis-local/Desktop/built.io/deepakWork/flickr8k_dataset/Flickr8k_text/Flickr_8k.trainImages.txt\", \"./features.pkl\")\n",
    "# prepare tokenizer and determine max length of desc\n",
    "tokenizer, max_length, vocab_size = create_tokenizer_max_length(train_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare sequences\n",
    "# X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X1train.shape, X2train.shape, ytrain.shape, ytrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare test dataset with startseq desc endseq.\n",
    "# test_descriptions, test_features  = load_clean_descriptions_photo_features(\"./descriptions.txt\", \"/home/gis-local/Desktop/built.io/deepakWork/flickr8k_dataset/Flickr8k_text/Flickr_8k.testImages.txt\", \"./features.pkl\")\n",
    "# prepare sequences\n",
    "# X1test, X2test, ytest, vocab_size = create_sequences(tokenizer, max_length, test_descriptions, test_features, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 34)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 34, 256)      1940224     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 4096)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 34, 256)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          1048832     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          525312      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256)          0           dense_1[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 7579)         1947803     dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,527,963\n",
      "Trainable params: 5,527,963\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = define_model(vocab_size, max_length)\n",
    "# define checkpoint callback\n",
    "# filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "# checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "# # fit model\n",
    "# model.fit([X1train, X2train], ytrain, epochs=1, verbose=2, callbacks=[checkpoint], validation_data=([X1test, X2test], ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runnning epochs: 0\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 1235s 206ms/step - loss: 4.6793\n",
      "runnning epochs: 1\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 1302s 217ms/step - loss: 3.9168\n",
      "runnning epochs: 2\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 1273s 212ms/step - loss: 3.6553\n",
      "runnning epochs: 3\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 1307s 218ms/step - loss: 3.5099\n",
      "runnning epochs: 4\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 1405s 234ms/step - loss: 3.4145\n",
      "runnning epochs: 5\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 1397s 233ms/step - loss: 3.3445\n",
      "runnning epochs: 6\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 1548s 258ms/step - loss: 3.2960\n",
      "runnning epochs: 7\n",
      "Epoch 1/1\n",
      "5330/6000 [=========================>....] - ETA: 3:24 - loss: 3.2621"
     ]
    }
   ],
   "source": [
    "# data generator, intended to be used in a call to model.fit_generator()\n",
    "def data_generator(descriptions, photos, tokenizer, max_length):\n",
    "    # loop for ever over images\n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            # retrieve the photo feature\n",
    "            photo = photos[key][0]\n",
    "            in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n",
    "            yield [[in_img, in_seq], out_word]\n",
    "            \n",
    "# run in progressive loading if you don't have enough memory\n",
    "# train the model, run epochs manually and save after each epoch\n",
    "epochs = 20\n",
    "steps = len(train_descriptions)\n",
    "for i in range(epochs):\n",
    "    print(\"runnning epochs:\", i)\n",
    "    # create the data generator\n",
    "    generator = data_generator(train_descriptions, train_features, tokenizer, max_length)\n",
    "    # fit for one epoch\n",
    "    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    # save model\n",
    "    model.save('model_' + str(i) + '.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
