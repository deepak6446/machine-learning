{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deepak/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from lr_utils import load_dataset\n",
    "#train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()\n",
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_dataset()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# linear --> relu --> linear --> relu --> linear --> sigmoid --> y output\n",
    "# repeat [linear --> relu] to L-1 times for L layer neural network\n",
    "# [linear --> relu] is one layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    shape of \\n    X = (12288,209)    (num_of_parameters, number_of_test_data)\\n    layer1 = (n[1],12288)  n[l]  is the number of units in layer  l\\n            .\\n            .\\n            .\\n    layerl = (n[L],n[L−1])\\n    b = (n[l], 1)  \\n    A=WX+b = (n[l],12288)*(12288,209) + (n[l], 1)  = (n[l], 209)  \\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now we will implement all helper function\n",
    "'''\n",
    "    shape of \n",
    "    X = (12288,209)    (num_of_parameters, number_of_test_data)\n",
    "    layer1 = (n[1],12288)  n[l]  is the number of units in layer  l\n",
    "            .\n",
    "            .\n",
    "            .\n",
    "    layerl = (n[L],n[L−1])\n",
    "    b = (n[l], 1)  \n",
    "    A=WX+b = (n[l],12288)*(12288,209) + (n[l], 1)  = (n[l], 209)  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "    ### END CODE HERE ###\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize_parameters for L-layer Neural Network\n",
    "# dim of layerl = (n[L],n[L−1])\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid activation and relu activation\n",
    "# sigmoid is used for binary classification\n",
    "# relu activation works best in non linear dataset and is fast for gradient decent when z is very large\n",
    "\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing \n",
    "# Z[l]=W[l]A[l−1]+b[l]   for l=0 A[l-1] = X\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    Z = np.dot(W, A) + b\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comman function for relu and sigmoid activation function\n",
    "# returns: A, cache: A,(linear_cache, activation_cache) : A, ((A_prev, W, b), Z)  \n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing L layer neural network\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A, cache = linear_activation_forward(A_prev,\n",
    "                                             parameters['W' + str(l)], \n",
    "                                             parameters['b' + str(l)], \n",
    "                                             activation='relu')\n",
    "        caches.append(cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    AL, cache = linear_activation_forward(A, \n",
    "                                          parameters['W' + str(L)], \n",
    "                                          parameters['b' + str(L)], \n",
    "                                          activation='sigmoid')\n",
    "    caches.append(cache)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute cost \n",
    "\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 lines of code)\n",
    "    cost = -(1  / m) * np.sum( np.multiply(Y, np.log(AL)) + np.multiply((1 - Y), np.log(1 - AL)) ) \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_backward\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = np.dot(dZ, cache[0].T) / m\n",
    "    db = np.squeeze(np.sum(dZ, axis=1, keepdims=True)) / m\n",
    "    dA_prev = np.dot(cache[1].T, dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    ##assert (isinstance(db, float))\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_activation_backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = relu_backward(dA, activation_cache)        \n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_backward\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    current_cache = caches[L - 1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        ### START CODE HERE ### (approx. 5 lines)\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we have parameters dw db for all hidden layers we can find optimal value of w and b\n",
    "# Update parameters using gradient descent\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 = np.random.randn(n_h, n_x)*0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h)*0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 1. It's a cat picture.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3da3b119e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a picture\n",
    "index = 11\n",
    "plt.imshow(train_x_orig[index])\n",
    "print (\"y = \" + str(train_y[0,index]) + \". It's a \" + classes[train_y[0,index]].decode(\"utf-8\") +  \" picture.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 209\n",
      "Number of testing examples: 50\n",
      "Each image is of size: (64, 64, 3)\n",
      "train_x_orig shape: (209, 64, 64, 3)\n",
      "train_y shape: (1, 209)\n",
      "test_x_orig shape: (50, 64, 64, 3)\n",
      "test_y shape: (1, 50)\n"
     ]
    }
   ],
   "source": [
    "# Explore your dataset \n",
    "m_train = train_x_orig.shape[0]\n",
    "num_px = train_x_orig.shape[1]\n",
    "m_test = test_x_orig.shape[0]\n",
    "\n",
    "print (\"Number of training examples: \" + str(m_train))\n",
    "print (\"Number of testing examples: \" + str(m_test))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_x_orig shape: \" + str(train_x_orig.shape))\n",
    "print (\"train_y shape: \" + str(train_y.shape))\n",
    "print (\"test_x_orig shape: \" + str(test_x_orig.shape))\n",
    "print (\"test_y shape: \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (12288, 209)\n",
      "test_x's shape: (12288, 50)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples \n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [12288, 20, 7, 5, 1] #  4-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization. (≈ 1 line of code)\n",
    "    ### START CODE HERE ###\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "        # Backward propagation.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        grads =  L_model_backward(AL, Y, caches)\n",
    "        ### END CODE HERE ###\n",
    " \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        parameters =  update_parameters(parameters, grads, learning_rate=learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12288, 209) (1, 209) [12288, 20, 7, 5, 1]\n",
      "(12288, 209) (1, 209) [12288, 20, 7, 5, 1]\n",
      "[0.06666667 0.76862745 0.32156863 0.00392157 0.03529412 0.32941176\n",
      " 0.21960784 0.0745098  0.24705882 0.09019608 0.7372549  0.01568627\n",
      " 0.60392157 0.06666667 0.28235294 0.96078431 0.99215686 0.85098039\n",
      " 0.54901961 0.00784314 0.01960784 0.06666667 0.64313725 0.61176471\n",
      " 0.47843137 0.05882353 0.30588235 0.14117647 0.05490196 0.70588235\n",
      " 0.15294118 0.74509804 0.91372549 0.50588235 0.5372549  0.10196078\n",
      " 0.09019608 0.36862745 0.24705882 0.44313725 0.46666667 0.00392157\n",
      " 0.24705882 1.         0.23921569 0.         0.25098039 0.2\n",
      " 0.08235294 0.22352941 0.64313725 0.59607843 0.41568627 0.15686275\n",
      " 0.05882353 1.         0.12156863 0.55294118 0.20392157 0.29411765\n",
      " 0.31764706 0.49019608 0.38823529 0.36862745 0.00784314 0.3372549\n",
      " 0.88627451 0.29803922 0.54509804 0.16862745 0.09411765 0.02745098\n",
      " 0.05098039 0.40392157 0.33333333 0.43137255 0.09803922 0.23921569\n",
      " 0.13333333 0.10588235 0.69019608 0.73333333 0.10196078 0.98823529\n",
      " 0.37647059 0.09803922 0.13333333 0.23529412 0.48235294 0.17647059\n",
      " 0.38823529 0.19215686 0.10196078 0.60392157 0.55294118 0.24313725\n",
      " 0.59607843 0.76078431 0.44313725 0.22352941 0.6745098  0.2745098\n",
      " 0.08627451 0.55686275 0.14509804 0.49803922 0.6745098  0.47843137\n",
      " 0.43137255 0.29411765 0.64705882 0.68235294 0.01960784 0.65098039\n",
      " 0.56470588 0.76862745 0.00784314 0.25098039 0.74509804 0.66666667\n",
      " 0.3372549  0.41568627 0.77647059 0.2745098  0.67058824 0.03529412\n",
      " 0.19607843 0.32941176 0.63137255 0.09019608 0.30980392 0.89411765\n",
      " 0.40784314 0.00392157 0.01960784 1.         0.55686275 0.76862745\n",
      " 0.52941176 0.34901961 0.         0.7372549  1.         0.06666667\n",
      " 0.12156863 0.6627451  0.53333333 0.30980392 0.50980392 0.58823529\n",
      " 0.98431373 0.02745098 0.17647059 0.62352941 0.03921569 0.52941176\n",
      " 0.1254902  0.11764706 0.54901961 0.11372549 0.11372549 0.43137255\n",
      " 0.38823529 0.94901961 0.61960784 0.11764706 0.94117647 0.32941176\n",
      " 0.03921569 0.36470588 0.78431373 0.74509804 0.52156863 0.29019608\n",
      " 0.09803922 0.01176471 0.41568627 0.52156863 0.04705882 0.41176471\n",
      " 0.9372549  0.00392157 0.24313725 0.2627451  0.11372549 0.69803922\n",
      " 0.26666667 0.21568627 0.78823529 0.76470588 0.56470588 0.98431373\n",
      " 0.50980392 0.2627451  0.03921569 0.         0.36470588 0.39607843\n",
      " 0.59215686 0.11372549 1.         0.16862745 0.4        0.36470588\n",
      " 0.78431373 0.03529412 0.56078431 0.08627451 0.03137255] [0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0\n",
      " 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 1 1\n",
      " 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0\n",
      " 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0\n",
      " 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0]\n",
      "Cost after iteration 0: 0.693148\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (20,209) (20,20) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-9e0091cf1d76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL_layer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-8b2226ecbb57>\u001b[0m in \u001b[0;36mL_layer_model\u001b[0;34m(X, Y, layers_dims, learning_rate, num_iterations, print_cost)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m### START CODE HERE ### (≈ 1 line of code)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL_model_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;31m### END CODE HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-aca0d2280241>\u001b[0m in \u001b[0;36mL_model_forward\u001b[0;34m(X, parameters)\u001b[0m\n\u001b[1;32m     25\u001b[0m                                              \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                                              \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                                              activation='relu')\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mcaches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m### END CODE HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-821bce9900c9>\u001b[0m in \u001b[0;36mlinear_activation_forward\u001b[0;34m(A_prev, W, b, activation)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m### START CODE HERE ### (≈ 2 lines of code)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m### END CODE HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-09b5e10dc51e>\u001b[0m in \u001b[0;36mlinear_forward\u001b[0;34m(A, W, b)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m### START CODE HERE ### (≈ 1 line of code)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;31m### END CODE HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (20,209) (20,20) "
     ]
    }
   ],
   "source": [
    "print('(12288, 209) (1, 209) [12288, 20, 7, 5, 1]')\n",
    "print(train_x.shape, train_y.shape, layers_dims)\n",
    "print(train_x[0], train_y[0])\n",
    "parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
